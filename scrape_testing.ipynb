{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns title and paragraph for news \n",
    "browser = init_browser()\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "news_title = soup.find(\"div\", class_='content_title').get_text()\n",
    "news_body = soup.find(\"div\", class_='article_teaser_body').get_text()\n",
    "news_title, news_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featured image\n",
    "url2 = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url2)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "featured_image = soup.find(class_='button fancybox')['data-fancybox-href']\n",
    "featured_image_url = (\"https://www.jpl.nasa.gov\" + featured_image)\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mars weather\n",
    "url3 = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(url3)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "weather = soup.find(\"p\", class_='tweet-text').get_text()\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns table from space facts \n",
    "url4 = \"https://space-facts.com/mars/\"\n",
    "table = pd.read_html(url4)\n",
    "df1 = table[0]\n",
    "df1.columns = ['Variable', 'Measurement']\n",
    "df1.set_index('Variable', inplace=True)\n",
    "space_facts = df1.to_html(classes=\"table\")\n",
    "space_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hemispheres info\n",
    "browser = init_browser()\n",
    "\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "hemispheres = []\n",
    "image_urls = []\n",
    "# loop to find hemisphere names\n",
    "for hemisphere in soup.find_all('h3'):\n",
    "    hemispheres.append(hemisphere.get_text())\n",
    "# loop to find image urls \n",
    "for hemisphere in hemispheres:\n",
    "    browser.click_link_by_partial_text(hemisphere)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html)\n",
    "    div = soup.find('div', class_='downloads')\n",
    "    list_ = div.find('li')\n",
    "    img_url = list_.a['href']\n",
    "    image_urls.append(img_url)\n",
    "    browser.back()\n",
    "hemisphere_image_urls = dict(zip(hemispheres, image_urls))\n",
    "h_1_title = list(hemisphere_image_urls.keys())[0]\n",
    "h_2_title = list(hemisphere_image_urls.keys())[1]\n",
    "h_3_title = list(hemisphere_image_urls.keys())[2]\n",
    "h_4_title = list(hemisphere_image_urls.keys())[3]\n",
    "h_1_img = list(hemisphere_image_urls.values())[0]\n",
    "h_1_img = list(hemisphere_image_urls.values())[1]\n",
    "h_1_img = list(hemisphere_image_urls.values())[2]\n",
    "h_1_img = list(hemisphere_image_urls.values())[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls\n",
    "h_1_title = list(hemisphere_image_urls.keys())[0]\n",
    "hemisphere_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    # returns title and paragraph for news \n",
    "    browser = init_browser()\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    news_title = soup.find(\"div\", class_='content_title').get_text()\n",
    "    news_body = soup.find(\"div\", class_='article_teaser_body').get_text()\n",
    "    # featured image\n",
    "    url2 = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(url2)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    featured_image = soup.find_all('img')[3][\"src\"]\n",
    "    featured_image_url = (\"https://www.jpl.nasa.gov\" + featured_image)\n",
    "    # mars weather\n",
    "    url3 = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    browser.visit(url3)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    weather = soup.find(\"p\", class_='tweet-text').get_text()\n",
    "    # returns table from space facts \n",
    "    url4 = \"https://space-facts.com/mars/\"\n",
    "    table = pd.read_html(url4)\n",
    "    df_facts = table[0]\n",
    "    space_facts = df_facts.to_html(classes=\"table\")\n",
    "    # hemisphere info \n",
    "    url5 = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    hemispheres = []\n",
    "    image_urls = []\n",
    "    # loop to find hemisphere names\n",
    "    for hemisphere in soup.find_all('h3'):\n",
    "        hemispheres.append(hemisphere.get_text())\n",
    "    # loop to find image urls \n",
    "    for hemisphere in hemispheres:\n",
    "        browser.click_link_by_partial_text(hemisphere)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html)\n",
    "        div = soup.find('div', class_='downloads')\n",
    "        list_ = div.find('li')\n",
    "        img_url = list_.a['href']\n",
    "        image_urls.append(img_url)\n",
    "        browser.back()\n",
    "    hemisphere_image_urls = dict(zip(hemispheres, image_urls))\n",
    "    h_1_title = list(hemisphere_image_urls.keys())[0]\n",
    "    h_2_title = list(hemisphere_image_urls.keys())[1]\n",
    "    h_3_title = list(hemisphere_image_urls.keys())[2]\n",
    "    h_4_title = list(hemisphere_image_urls.keys())[3]\n",
    "    h_1_img = list(hemisphere_image_urls.values())[0]\n",
    "    h_2_img = list(hemisphere_image_urls.values())[1]\n",
    "    h_3_img = list(hemisphere_image_urls.values())[2]\n",
    "    h_4_img = list(hemisphere_image_urls.values())[3]\n",
    "    mars_data = {\"news_title\": news_title,\n",
    "             \"news_body\": news_body,\n",
    "             \"featured_image_url\": featured_image_url,\n",
    "             \"weather\": weather,\n",
    "             \"space_facts\": space_facts,\n",
    "             \"h_1_title\": h_1_title, \n",
    "             \"h_2_title\": h_2_title,\n",
    "             \"h_3_title\": h_3_title,\n",
    "             \"h_4_title\": h_4_title,\n",
    "             \"h_1_img\": h_1_img,\n",
    "             \"h_2_img\": h_2_img,\n",
    "             \"h_3_img\": h_3_img,\n",
    "             \"h_4_img\": h_4_img\n",
    "                }\n",
    "    return mars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turtle.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mandelbrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
